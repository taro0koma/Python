{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# スクレイピングに挑戦\n",
    "## HTMLテキストを出力（エンコード調整）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html>\n",
      "\t<head>\n",
      "\t\t<meta charset=\"UTF-8\">\n",
      "\t\t<title>Python2年生</title>\n",
      "\t</head>\n",
      "\t<body>\n",
      "\t\t<h2>第1章 Pythonでデータをダウンロード</h2>\n",
      "\t\t<ol>\n",
      "\t\t\t<li>スクレイピングってなに？</li>\n",
      "\t\t\t<li>Pythonをインストールしてみよう</li>\n",
      "\t\t\t<li>requestsでアクセスしてみよう</li>\n",
      "\t\t</ol>\n",
      "\t</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url=\"https://www.ymori.com/books/python2nen/test1.html\"\n",
    "response=requests.get(url)  # requestsライブラリのgetメソッドを使って、指定URLのwebページを取得する\n",
    "response.encoding=response.apparent_encoding  # 出力文字コードを自動的に選択指定\n",
    "print(response.text)  # 取得したwebページをテキスト化し、文字列データの取得"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ファイル書込みの処理\n",
    "\n",
    "#### ◆open()関数のモード<br>\n",
    "r   読み込み専用（デフォルト）<br>\n",
    "w   書き込み専用<br>\n",
    "a   上書き\n",
    "<br>-----------------<br>\n",
    "t   テキストモード<br>\n",
    "b   バイナリモード\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url=\"https://www.ymori.com/books/python2nen/test1.html\"\n",
    "response=requests.get(url)  # requestsライブラリのgetメソッドを使って、指定URLのwebページを取得する\n",
    "response.encoding=response.apparent_encoding  # 出力文字コードを自動的に選択指定\n",
    "\n",
    "# f=open(filename,mode=\"w\")  新規ファイルを書き込みモードで開く\n",
    "# f.write(書き込む値)  データを書き込む\n",
    "# f.close()  ファイルを閉じる\n",
    "\n",
    "filename=\"download.txt\"\n",
    "f=open(filename,mode=\"w\")\n",
    "\n",
    "# 閉じ忘れ防止↑\n",
    "# with-as文(f=ではなく、as fと書く)\n",
    "# with open(filename,mode=\"w\") as f:\n",
    "#      f.write(書込み値)\n",
    "# try文のように「開いたら書く、指定処理が終わったら閉じる」を一括規定できる\n",
    "\n",
    "f.write(response.text)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BeautifulSoupを使ってHTMLを解析する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "\n",
      "<html>\n",
      "<head>\n",
      "<meta charset=\"utf-8\"/>\n",
      "<title>Python2年生</title>\n",
      "</head>\n",
      "<body>\n",
      "<h2>第1章 Pythonでデータをダウンロード</h2>\n",
      "<ol>\n",
      "<li>スクレイピングってなに？</li>\n",
      "<li>Pythonをインストールしてみよう</li>\n",
      "<li>requestsでアクセスしてみよう</li>\n",
      "</ol>\n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs  # bs4パッケージからインストールするのでfrom文を使う\n",
    "\n",
    "# webページを取得して解析する\n",
    "load_url=\"https://www.ymori.com/books/python2nen/test1.html\"\n",
    "# requestsライブラリのget()メソッドによって指定urlが指すHTMLデータを取得する\n",
    "html=requests.get(load_url)\n",
    "#　HTML内部（contentフィールド）をBeautifulSoupのhtml.parserで解析する\n",
    "soup=bs(html.content,\"html.parser\")\n",
    "# HTML全体を表示する（html.textをそのまま出力した物とは異なる）\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML内のコードタグを抽出する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Python2年生</title>\n",
      "<h2>第1章 Pythonでデータをダウンロード</h2>\n",
      "<li>スクレイピングってなに？</li>\n",
      "\n",
      "Python2年生\n",
      "第1章 Pythonでデータをダウンロード\n",
      "スクレイピングってなに？\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "# webページ　取得解析\n",
    "load_url=\"https://www.ymori.com/books/python2nen/test1.html\"\n",
    "html=requests.get(load_url)\n",
    "soup=bs(html.content,\"html.parser\")\n",
    "\n",
    "# 指定HTMLタグ囲い文を検索／抽出　対象→<title><h2><li>\n",
    "print(soup.find(\"title\"))\n",
    "print(soup.find(\"h2\"))\n",
    "print(soup.find(\"li\"))\n",
    "\n",
    "print()\n",
    "# 抽出したタグ囲い文内　タグ内の有効テキストのみ抽出 findタグのテキスト抽出\n",
    "print(soup.find(\"title\").text)\n",
    "print(soup.find(\"h2\").text)\n",
    "print(soup.find(\"li\").text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find_all()して複数抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "スクレイピングってなに？\n",
      "Pythonをインストールしてみよう\n",
      "requestsでアクセスしてみよう\n",
      "HTMLを解析してみよう\n",
      "ニュースの最新記事一覧を取得してみよう\n",
      "リンク一覧をファイルに書き出そう\n",
      "画像を一括ダウンロードしよう\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "load_url=\"https://www.ymori.com/books/python2nen/test2.html\"\n",
    "html=requests.get(load_url)\n",
    "soup=bs(html.content,\"html.parser\")\n",
    "\n",
    "# find()メソッドは同一タグが複数存在しても初出のみ抽出\n",
    "# find_all()メソッドによって、すべての同一タグを抽出する\n",
    "for element in soup.find_all(\"li\"):\n",
    "    print(element.text)  # 指定タグをfind_allしてtextフィールドのみ抽出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "idやclassなど、属性指定で抽出する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div id=\"chap2\">\n",
      "<h2>第2章 HTMLを解析しよう</h2>\n",
      "<ol>\n",
      "<li>HTMLを解析してみよう</li>\n",
      "<li>ニュースの最新記事一覧を取得してみよう</li>\n",
      "<li>リンク一覧をファイルに書き出そう</li>\n",
      "<li>画像を一括ダウンロードしよう</li>\n",
      "</ol>\n",
      "</div>\n",
      "\n",
      "第2章 HTMLを解析しよう\n",
      "\n",
      "HTMLを解析してみよう\n",
      "ニュースの最新記事一覧を取得してみよう\n",
      "リンク一覧をファイルに書き出そう\n",
      "画像を一括ダウンロードしよう\n",
      "\n",
      "\n",
      "HTMLを解析してみよう\n",
      "ニュースの最新記事一覧を取得してみよう\n",
      "リンク一覧をファイルに書き出そう\n",
      "画像を一括ダウンロードしよう\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "# webページ取得・解析\n",
    "load_url=\"https://www.ymori.com/books/python2nen/test2.html\"\n",
    "html=requests.get(load_url)\n",
    "soup=bs(html.content,\"html.parser\")\n",
    "\n",
    "# タグIDを指定して抽出\n",
    "chap2=soup.find(id=\"chap2\")\n",
    "print(chap2)\n",
    "print(chap2.text)\n",
    "\n",
    "# chap2に絞ったHTMLコードから<li>部分を抽出する\n",
    "for element in chap2.find_all(\"li\"):\n",
    "    print(element.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 大規模サイトで抽出に挑戦\n",
    "デベロッパーツールを使って絞り込む"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "スマホ圏外の遭難者特定?実験\n",
      "https://news.yahoo.co.jp/pickup/6351606\n",
      "巨大IT規制、来春にも強化\n",
      "https://news.yahoo.co.jp/pickup/6351581\n",
      "Apple失速 日本にも飛び火か\n",
      "https://news.yahoo.co.jp/pickup/6351579\n",
      "サイバー攻撃 脆弱性悪用急増\n",
      "https://news.yahoo.co.jp/pickup/6351559\n",
      "肺炎 中国はネットで授業再開\n",
      "https://news.yahoo.co.jp/pickup/6351497\n",
      "国内大手初 auの残価設定型\n",
      "https://news.yahoo.co.jp/pickup/6351492\n",
      "シャープ 日本初5Gスマホ発表\n",
      "https://news.yahoo.co.jp/pickup/6351459\n",
      "米携帯3位と4位の合併 実現へ\n",
      "https://news.yahoo.co.jp/pickup/6351442\n"
     ]
    }
   ],
   "source": [
    "# yahooニュースサイト内HTMLクラス　class=\"topicList_main\"の部分を抽出する\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "# webページ　取得・解析\n",
    "load_url=\"https://news.yahoo.co.jp/categories/it\"\n",
    "html=requests.get(load_url)\n",
    "soup=bs(html.content,\"html.parser\")\n",
    "\n",
    "# class=\"topicList_main\"を検索\n",
    "topic=soup.find(class_=\"topicsList_main\")\n",
    "for element in topic.find_all(\"a\"):\n",
    "    print(element.text)  # タグ内テキストを抽出\n",
    "    print(element.get(\"href\"))  # タグ内ソースを抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "リンク1\n",
      "https://www.ymori.com/books/python2nen/test1.html\n",
      "リンク2\n",
      "./test3.html\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "# webページを取得・解析\n",
    "load_url=\"https://www.ymori.com/books/python2nen/test2.html\"\n",
    "html=requests.get(load_url)\n",
    "soup=bs(html.content,\"html.parser\")\n",
    "\n",
    "# すべての<a>タグを検索　href=ソースのみ抽出\n",
    "for element in soup.find_all(\"a\"):\n",
    "    print(element.text)\n",
    "    url=element.get(\"href\")\n",
    "    print(url)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相対パスも絶対パスで取得したい<br>\n",
    "urlibライブラリーをインポートしてお手伝いしてもらう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.BeautifulSoup'>\n",
      "リンク1\n",
      "https://www.ymori.com/books/python2nen/test1.html\n",
      "リンク2\n",
      "https://www.ymori.com/books/python2nen/test3.html\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import urllib\n",
    "\n",
    "# webページ 取得・解析\n",
    "load_url=\"https://www.ymori.com/books/python2nen/test2.html\"\n",
    "html=requests.get(load_url)\n",
    "soup=bs(html.content,\"html.parser\")  # BeautifulSoupオブジェクトを作る\n",
    "print(type(soup))\n",
    "\n",
    "# すべてのaタグを検索し、リンクを「絶対パス」で抽出する\n",
    "for element in soup.find_all(\"a\"):  # <a>タグの総ざらい\n",
    "    print(element.text)\n",
    "    url=element.get(\"href\")\n",
    "    link_url=urllib.parse.urljoin(load_url,url)  # 各<a>タグのhref属性リスト各要素にページurlをjoinする\n",
    "    print(link_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import urllib\n",
    "\n",
    "load_url=\"https://www.ymori.com/books/python2nen/test2.html\"\n",
    "html=requests.get(load_url)  # requestsオブジェクト作成　contentフィールドにhtmlコードを記録する\n",
    "soup=bs(html.content,\"html.parser\")\n",
    "\n",
    "# ファイルを書き込みモードで作成＆オープン\n",
    "filename=\"linklist.txt\"\n",
    "with open(filename,\"w\") as f:  # with文内で代入処理をする f=open(filename,\"w\") ファイルオブジェクトを作成\n",
    "    \n",
    "    # すべての<a>タグを抽出（リスト化）\n",
    "    # <a>タグのリストからhref属性記述を抽出（リスト化）\n",
    "    # href属性リストをすべて絶対パスに（各パスごとURLを精査し、相対パスは絶対パスに書き換える。）\n",
    "    # urllib.parse.urljoin\n",
    "    # 一つの<a>タグにつき、①文字列ブロック　②絶対パス　③空白行　の出力を行う\n",
    "    \n",
    "    for element in soup.find_all(\"a\"):\n",
    "        url=element.get(\"href\")\n",
    "        link_url=urllib.parse.urljoin(load_url,url)\n",
    "        f.write(element.text+\"\\n\")\n",
    "        f.write(link_url+\"\\n\")\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
